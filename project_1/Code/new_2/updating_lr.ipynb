{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe8f83c",
   "metadata": {},
   "source": [
    "# Kodekladd\n",
    "\n",
    "According to Week 36:\n",
    "\n",
    "OLS:\n",
    "$$\n",
    "\\nabla_{\\theta} C(\\theta) = \\frac{2}{n}X^T(X\\theta - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "Ridge:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+2\\lambda \\theta\n",
    "$$\n",
    "\n",
    "Lasso:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+\\lambda sgn(\\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b507c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import OLS_gradient, Ridge_gradient, polynomial_features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9a34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "np.random.seed(42)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "lr = 0.01\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "np.savez('data_arrays_last.npz', X_train_s=X_train_s, X_test_s=X_test_s, y_test=y_test, y_train=y_train, y_offset=y_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f4610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached at iteration for Ridge 358451\n",
      "Learning rate: 0.01\n",
      "MSE GD OLS: 0.0014844050649363589, MSE GD Ridge: 0.009755930977492997\n",
      "R2 GD OLS: 0.9800068691929409, R2 GD Ridge: 0.8688668204969766\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74857078e-03 -2.97354776e+00  9.64382499e-03\n",
      "  1.25083194e+01 -7.08558248e-02 -2.40861275e+01  1.12139439e-01\n",
      "  2.15432490e+01 -5.43688960e-02 -7.24137358e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891847e-04 -1.02262379e+00 -6.90114500e-03\n",
      "  1.21924449e+00  6.72252475e-03 -4.72616115e-02  5.79556250e-03\n",
      " -3.91830338e-01 -7.44610701e-03  1.65154352e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Addition of momentum\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "num_iters = 100000000\n",
    "momentum = 0.3\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "change = 0.0\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    # Calculate change with momentum\n",
    "    new_change = lr * grad_Ridge + momentum * change\n",
    "    # Update parameters beta\n",
    "    beta_gd_r = beta_gd_r - new_change\n",
    "    # Save change for next iteration\n",
    "    change = new_change\n",
    "    # Check for convergence\n",
    "    if (np.abs(new_change) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "change = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    # Calculate change with momentum\n",
    "    new_change = lr * grad_OLS + momentum * change\n",
    "    # Update parameters beta\n",
    "    beta_gd_o = beta_gd_o - new_change\n",
    "    # Save change for next iteration\n",
    "    change = new_change\n",
    "    # Check for convergence\n",
    "    if (np.abs(new_change) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_momentum = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('momentum_results.json', 'w') as f:\n",
    "    json.dump(dict_momentum, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c28de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached at iteration for Ridge 369750\n",
      "Learning rate: 0.01\n",
      "MSE GD OLS: 0.0014845180569028782, MSE GD Ridge: 0.009755932193597897\n",
      "R2 GD OLS: 0.980005351916389, R2 GD Ridge: 0.8688668041923406\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74423783e-03 -2.97412571e+00  9.68415136e-03\n",
      "  1.25125987e+01 -7.09926302e-02 -2.40972296e+01  1.12322150e-01\n",
      "  2.15552046e+01 -5.44518438e-02 -7.24593821e+00], Beta GD Ridge: [ 0.00000000e+00  1.91887126e-04 -1.02262366e+00 -6.90113717e-03\n",
      "  1.21924431e+00  6.72254380e-03 -4.72619020e-02  5.79551035e-03\n",
      " -3.91829741e-01 -7.44607680e-03  1.65151868e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ADAgrad\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "G_iter = 0.0\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    G_iter += grad_Ridge*grad_Ridge\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_Ridge\n",
    "    beta_gd_r = beta_gd_r - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "G_iter = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    G_iter += grad_OLS*grad_OLS\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_OLS\n",
    "    beta_gd_o = beta_gd_o - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_adagrad = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('adagrad_results.json', 'w') as f:\n",
    "    json.dump(dict_adagrad, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a09fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.01\n",
      "MSE GD OLS: 0.0023747134340582495, MSE GD Ridge: 0.010707072948244595\n",
      "R2 GD OLS: 0.9680245144006304, R2 GD Ridge: 0.855978969899131\n",
      "Beta GD OLS: [ 0.00000000e+00 -1.25643651e-03 -2.96926631e+00  4.69057174e-03\n",
      "  1.25186327e+01 -7.60156112e-02 -2.40949002e+01  1.07354141e-01\n",
      "  2.15630712e+01 -5.94668213e-02 -7.24203004e+00], Beta GD Ridge: [ 0.00000000e+00 -4.80810701e-03 -1.01762437e+00 -1.19011362e-02\n",
      "  1.22424696e+00  1.72253834e-03 -4.22647965e-02  7.95507315e-04\n",
      " -3.86829455e-01 -1.24460675e-02  2.15158418e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#RMSprop\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "G_iter = np.zeros(len(beta_r))\n",
    "rho = 0.9\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    G_iter = rho*G_iter + (1-rho)*grad_Ridge*grad_Ridge\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_Ridge\n",
    "    beta_gd_r = beta_gd_r - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "G_iter = np.zeros(len(beta_o))\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    G_iter = (rho*G_iter + (1-rho)*grad_OLS*grad_OLS)\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_OLS\n",
    "    beta_gd_o = beta_gd_o - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_rmsprop = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('rmsprop_results.json', 'w') as f:\n",
    "    json.dump(dict_rmsprop, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91fcb8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached at iteration for Ridge 5182022\n",
      "Convergence reached at iteration for OLS 2175289\n",
      "Learning rate: 0.01\n",
      "MSE GD OLS: 0.0014844716177567874, MSE GD Ridge: 0.009755593889671099\n",
      "R2 GD OLS: 0.9800059441048328, R2 GD Ridge: 0.8688718153835356\n",
      "Beta GD OLS: [ 0.00000000e+00  3.78450056e-03 -2.97430724e+00  9.73150898e-03\n",
      "  1.25135918e+01 -7.09746739e-02 -2.40999412e+01  1.12395078e-01\n",
      "  2.15580303e+01 -5.44258841e-02 -7.24707098e+00], Beta GD Ridge: [ 0.00000000e+00  2.14372859e-04 -1.02264685e+00 -6.87865622e-03\n",
      "  1.21922448e+00  6.74501838e-03 -4.72872765e-02  5.81798735e-03\n",
      " -3.91851935e-01 -7.42358753e-03  1.64933618e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ADAM\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho_1 = 0.9\n",
    "rho_2 = 0.99\n",
    "first_moment = 0.0\n",
    "second_moment = 0.0\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    t += 1\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    # Computing moments first\n",
    "    first_moment = rho_1 * first_moment + (1 - rho_1) * grad_Ridge\n",
    "    second_moment = rho_2 * second_moment + (1 - rho_2) * grad_Ridge * grad_Ridge\n",
    "    first_term = first_moment / (1 - rho_1**(t))\n",
    "    second_term = second_moment / (1 - rho_2**(t))\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "    beta_gd_r = beta_gd_r - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "first_moment = 0.0\n",
    "second_moment = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    t += 1\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    # Computing moments first\n",
    "    first_moment = rho_1 * first_moment + (1 - rho_1) * grad_OLS\n",
    "    second_moment = rho_2 * second_moment + (1 - rho_2) * grad_OLS * grad_OLS\n",
    "    first_term = first_moment / (1 - rho_1**(t))\n",
    "    second_term = second_moment / (1 - rho_2**(t))\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "    beta_gd_o = beta_gd_o - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_adam = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('adam_results.json', 'w') as f:\n",
    "    json.dump(dict_adam, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
