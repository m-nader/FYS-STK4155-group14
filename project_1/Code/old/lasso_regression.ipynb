{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe8f83c",
   "metadata": {},
   "source": [
    "# Kodekladd\n",
    "\n",
    "According to Week 36:\n",
    "\n",
    "OLS:\n",
    "$$\n",
    "\\nabla_{\\theta} C(\\theta) = \\frac{2}{n}X^T(X\\theta - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "Ridge:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+2\\lambda \\theta\n",
    "$$\n",
    "\n",
    "Lasso:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+\\lambda sgn(\\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b507c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import Lasso_gradient, OLS_gradient, Ridge_gradient, polynomial_features\n",
    "from sklearn import linear_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b8655ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "Learning rate: 0.12791189798390853\n",
      "MSE GD Lasso: 0.007934752821654723\n",
      "R2 GD Lasso: 0.893283644201314\n",
      "Beta GD Lasso: [ 0.00000000e+00  6.12211598e-06 -1.21675310e+00 -4.10101384e-04\n",
      "  1.93552300e+00 -1.09601575e-04 -9.49776049e-01 -4.37405611e-05\n",
      " -1.16835864e-04 -2.58812997e-04  1.23675786e-04]\n",
      "--------------------------------------------------\n",
      "MSE Lasso Scikit-Learn: 0.009931854710884884\n",
      "R2 Lasso Scikit-Learn: 0.8665146819730404\n",
      "Beta Lasso Scikit-Learn: [ 0.00000000e+00 -4.06494043e-04 -1.00759002e+00 -0.00000000e+00\n",
      "  1.16444503e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -3.81997637e-01 -0.00000000e+00 -0.00000000e+00] \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Lasso with GD\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Lasso\n",
    "    grad_Lasso = Lasso_gradient(X_train_s, y_train, beta_gd_lasso, lmbda)\n",
    "    # Update parameters beta\n",
    "    beta_gd_lasso = beta_gd_lasso - lr * grad_Lasso\n",
    "    # Check for convergence\n",
    "    if (np.abs(- lr * grad_Lasso) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Lasso\", t)\n",
    "        break\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD Lasso: {MSE(y_test, y_gd_lasso)}\")\n",
    "print(f\"R2 GD Lasso: {R2(y_test, y_gd_lasso)}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_lasso_plain_gd = {'MSE GD Lasso': MSE(y_test, y_gd_lasso),\n",
    "                       'R2 GD Lasso': R2(y_test, y_gd_lasso),\n",
    "                       'Beta GD Lasso': beta_gd_lasso,}\n",
    "with open('lasso_plain_gd_results.json', 'w') as f:\n",
    "    json.dump(dict_lasso_plain_gd, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)\n",
    "\n",
    "# include lasso using Scikit-Learn\n",
    "RegLasso = linear_model.Lasso(lmbda,fit_intercept=False, max_iter=10000000)\n",
    "RegLasso.fit(X_train_s,y_train)\n",
    "y_lasso_sklearn = RegLasso.predict(X_test_s) + y_offset\n",
    "mse_lasso_sklearn = MSE(y_test, y_lasso_sklearn)\n",
    "r2_lasso_sklearn = R2(y_test, y_lasso_sklearn)\n",
    "print(f\"MSE Lasso Scikit-Learn: {mse_lasso_sklearn}\")\n",
    "print(f\"R2 Lasso Scikit-Learn: {r2_lasso_sklearn}\")\n",
    "print(f\"Beta Lasso Scikit-Learn: {RegLasso.coef_} \")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_lasso_sklearn = {'MSE': mse_lasso_sklearn,\n",
    "                      'R2': r2_lasso_sklearn,\n",
    "                      'Beta': RegLasso.coef_,}\n",
    "with open('lasso_sklearn_results.json', 'w') as f:\n",
    "    json.dump(dict_lasso_sklearn, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f4610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "Learning rate: 0.12791189798390853\n",
      "MSE GD Lasso: 0.007934224673617768\n",
      "R2 GD Lasso: 0.8932906495136685\n",
      "Beta GD Lasso: [ 0.00000000e+00 -7.57911899e-06 -1.21670308e+00 -3.68643214e-04\n",
      "  1.93558629e+00 -1.05223176e-04 -9.49707198e-01 -9.74198273e-05\n",
      " -1.44871451e-04 -2.46502864e-04  4.15480986e-05]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Lasso with GD and momentum\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "momentum = 0.3\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "change = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Lasso\n",
    "    grad_Lasso = Lasso_gradient(X_train_s, y_train, beta_gd_lasso, lmbda)\n",
    "    # Calculate change with momentum\n",
    "    new_change = lr * grad_Lasso + momentum * change\n",
    "    # Update parameters beta\n",
    "    beta_gd_lasso = beta_gd_lasso - new_change\n",
    "    # Save change for next iteration\n",
    "    change = new_change\n",
    "    # Check for convergence\n",
    "    if (np.abs(- lr * grad_Lasso) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Lasso\", t)\n",
    "        break\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD Lasso: {MSE(y_test, y_gd_lasso)}\")\n",
    "print(f\"R2 GD Lasso: {R2(y_test, y_gd_lasso)}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_lasso_momentum = {'MSE GD Lasso': MSE(y_test, y_gd_lasso),\n",
    "                       'R2 GD Lasso': R2(y_test, y_gd_lasso),\n",
    "                       'Beta GD Lasso': beta_gd_lasso,}\n",
    "with open('lasso_momentum_results.json', 'w') as f:\n",
    "    json.dump(dict_lasso_momentum, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e685df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "Learning rate: 0.12791189798390853\n",
      "MSE GD Lasso: 0.007935177537664118\n",
      "R2 GD Lasso: 0.8932779672450609\n",
      "Beta GD Lasso: [ 0.00000000e+00  5.86753916e-07 -1.21672941e+00 -4.89667917e-04\n",
      "  1.93544673e+00 -7.92736280e-05 -9.49713742e-01 -4.89517761e-05\n",
      " -1.24026829e-05 -2.14103633e-04  1.98979754e-05]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Lasso with GD and ADAgrad\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "G_iter = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Lasso\n",
    "    grad_Lasso = Lasso_gradient(X_train_s, y_train, beta_gd_lasso, lmbda)\n",
    "    G_iter += grad_Lasso*grad_Lasso\n",
    "    # Update parameters beta\n",
    "    beta_gd_lasso = beta_gd_lasso - (lr / (np.sqrt(G_iter) + delta)) * grad_Lasso\n",
    "    # Check for convergence\n",
    "    if (np.abs(- lr * grad_Lasso) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Lasso\", t)\n",
    "        break\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD Lasso: {MSE(y_test, y_gd_lasso)}\")\n",
    "print(f\"R2 GD Lasso: {R2(y_test, y_gd_lasso)}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_lasso_adagrad = {'MSE GD Lasso': MSE(y_test, y_gd_lasso),\n",
    "                       'R2 GD Lasso': R2(y_test, y_gd_lasso),\n",
    "                       'Beta GD Lasso': beta_gd_lasso,}\n",
    "with open('lasso_adagrad_results.json', 'w') as f:\n",
    "    json.dump(dict_lasso_adagrad, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fab937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "Learning rate: 0.12791189798390853\n",
      "MSE GD Lasso: 0.15194912075588932\n",
      "R2 GD Lasso: -1.0370909751652015\n",
      "Beta GD Lasso: [ 0.00000000e+00 -6.15210939e-02 -1.17298851e+00 -6.65938524e-02\n",
      "  2.07765559e+00 -9.38320620e-02 -9.43301064e-01 -1.80376294e-04\n",
      " -1.49720912e-04 -9.91054353e-02  1.27882933e-01]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Lasso with GD and RMSprop\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "G_iter = 0.0\n",
    "rho = 0.99\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Lasso\n",
    "    grad_Lasso = Lasso_gradient(X_train_s, y_train, beta_gd_lasso, lmbda)\n",
    "    G_iter = (rho*G_iter + (1-rho)*grad_Lasso*grad_Lasso)\n",
    "    # Update parameters beta\n",
    "    beta_gd_lasso = beta_gd_lasso - (lr / (np.sqrt(G_iter) + delta)) * grad_Lasso\n",
    "    # Check for convergence\n",
    "    if (np.abs(- lr * grad_Lasso) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Lasso\", t)\n",
    "        break\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD Lasso: {MSE(y_test, y_gd_lasso)}\")\n",
    "print(f\"R2 GD Lasso: {R2(y_test, y_gd_lasso)}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_lasso_rmsprop = {'MSE GD Lasso': MSE(y_test, y_gd_lasso),\n",
    "                       'R2 GD Lasso': R2(y_test, y_gd_lasso),\n",
    "                       'Beta GD Lasso': beta_gd_lasso,}\n",
    "with open('lasso_rmsprop_results.json', 'w') as f:\n",
    "    json.dump(dict_lasso_rmsprop, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186f5a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "Learning rate: 0.12791189798390853\n",
      "MSE GD Lasso: 0.008175634013665483\n",
      "R2 GD Lasso: 0.890008803673024\n",
      "Beta GD Lasso: [ 0.00000000e+00 -1.96862127e-03 -1.21519143e+00 -5.08174300e-03\n",
      "  1.94167617e+00 -9.74012717e-04 -9.50608595e-01 -1.01515744e-03\n",
      " -7.13570441e-03 -4.67197294e-03  9.79433103e-03]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Lasso with GD and ADAM\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho_1 = 0.9\n",
    "rho_2 = 0.999\n",
    "first_moment = 0.0\n",
    "second_moment = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    t += 1\n",
    "    # Compute gradients for Lasso\n",
    "    grad_Lasso = Lasso_gradient(X_train_s, y_train, beta_gd_lasso, lmbda)\n",
    "    # Computing moments first\n",
    "    first_moment = rho_1 * first_moment + (1 - rho_1) * grad_Lasso\n",
    "    second_moment = rho_2 * second_moment + (1 - rho_2) * grad_Lasso * grad_Lasso\n",
    "    first_term = first_moment / (1 - rho_1**(t+1))\n",
    "    second_term = second_moment / (1 - rho_2**(t+1))\n",
    "    # Update parameters beta\n",
    "    beta_gd_lasso = beta_gd_lasso - (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "    # Check for convergence\n",
    "    if (np.abs(- lr * grad_Lasso) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Lasso\", t)\n",
    "        break\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD Lasso: {MSE(y_test, y_gd_lasso)}\")\n",
    "print(f\"R2 GD Lasso: {R2(y_test, y_gd_lasso)}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_lasso_adam = {'MSE GD Lasso': MSE(y_test, y_gd_lasso),\n",
    "                   'R2 GD Lasso': R2(y_test, y_gd_lasso),\n",
    "                   'Beta GD Lasso': beta_gd_lasso,}\n",
    "with open('lasso_adam_results.json', 'w') as f:\n",
    "    json.dump(dict_lasso_adam, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
