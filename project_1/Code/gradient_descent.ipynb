{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe8f83c",
   "metadata": {},
   "source": [
    "# Kodekladd\n",
    "\n",
    "According to Week 36:\n",
    "\n",
    "OLS:\n",
    "$$\n",
    "\\nabla_{\\theta} C(\\theta) = \\frac{2}{n}X^T(X\\theta - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "Ridge:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+2\\lambda \\theta\n",
    "$$\n",
    "\n",
    "Lasso:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+\\lambda sgn(\\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b507c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import OLS_gradient, Ridge_gradient, polynomial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge parameters: [ 0.00000000e+00  5.54300134e-03 -2.59074814e+00 -7.44146292e-03\n",
      "  9.69505866e+00 -9.50339784e-03 -1.68243883e+01  2.65540645e-02\n",
      "  1.37520890e+01 -1.42341245e-02 -4.27508408e+00]\n",
      "OLS parameters: [ 0.00000000e+00  3.74356233e-03 -2.97426630e+00  9.69057072e-03\n",
      "  1.25136327e+01 -7.10156122e-02 -2.40999002e+01  1.12354140e-01\n",
      "  2.15580712e+01 -5.44668224e-02 -7.24703004e+00]\n",
      "Convergence reached at iteration for Ridge 496103\n",
      "Learning rate: 0.01\n",
      "MSE OLS: 0.001484546631866829, MSE Ridge: 0.0017395603039358032, MSE GD OLS: 0.001483336902967267, MSE GD Ridge: 0.00975593448081572\n",
      "R2 OLS: 0.9800049681741101, R2 Ridge: 0.976567517228484, R2 GD OLS: 0.980021209527389, R2 GD Ridge: 0.8688667735767661\n",
      "Beta GD OLS: [ 0.00000000e+00  3.78952978e-03 -2.96767152e+00  9.26153929e-03\n",
      "  1.24648671e+01 -6.95490864e-02 -2.39734940e+01  1.10383619e-01\n",
      "  2.14220328e+01 -5.35680562e-02 -7.19511517e+00], Beta GD Ridge: [ 0.00000000e+00  1.91892035e-04 -1.02262351e+00 -6.90115050e-03\n",
      "  1.21924335e+00  6.72252324e-03 -4.72602678e-02  5.79558509e-03\n",
      " -3.91830521e-01 -7.44612405e-03  1.65151350e-02]\n",
      "--------------------------------------------------\n",
      "Convergence reached at iteration for Ridge 60053\n",
      "Convergence reached at iteration for OLS 22675175\n",
      "Learning rate: 0.1\n",
      "MSE OLS: 0.001484546631866829, MSE Ridge: 0.0017395603039358032, MSE GD OLS: 0.0014845447700272435, MSE GD Ridge: 0.009755924586889855\n",
      "R2 OLS: 0.9800049681741101, R2 Ridge: 0.976567517228484, R2 GD OLS: 0.9800049931762356, R2 GD Ridge: 0.868866906088705\n",
      "Beta GD OLS: [ 0.00000000e+00  3.78952978e-03 -2.96767152e+00  9.26153929e-03\n",
      "  1.24648671e+01 -6.95490864e-02 -2.39734940e+01  1.10383619e-01\n",
      "  2.14220328e+01 -5.35680562e-02 -7.19511517e+00], Beta GD Ridge: [ 0.00000000e+00  1.91892035e-04 -1.02262351e+00 -6.90115050e-03\n",
      "  1.21924335e+00  6.72252324e-03 -4.72602678e-02  5.79558509e-03\n",
      " -3.91830521e-01 -7.44612405e-03  1.65151350e-02]\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74362765e-03 -2.97425693e+00  9.68996101e-03\n",
      "  1.25135634e+01 -7.10135281e-02 -2.40997206e+01  1.12351339e-01\n",
      "  2.15578779e+01 -5.44655451e-02 -7.24695626e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891734e-04 -1.02262430e+00 -6.90113722e-03\n",
      "  1.21924665e+00  6.72253352e-03 -4.72643298e-02  5.79551506e-03\n",
      " -3.91829683e-01 -7.44607374e-03  1.65158504e-02]\n",
      "--------------------------------------------------\n",
      "Convergence reached at iteration for Ridge 31626\n",
      "Convergence reached at iteration for OLS 12007583\n",
      "Learning rate: 0.2\n",
      "MSE OLS: 0.001484546631866829, MSE Ridge: 0.0017395603039358032, MSE GD OLS: 0.0014845457008710596, MSE GD Ridge: 0.009755924146034281\n",
      "R2 OLS: 0.9800049681741101, R2 Ridge: 0.976567517228484, R2 GD OLS: 0.980004980676195, R2 GD Ridge: 0.8688669119935727\n",
      "Beta GD OLS: [ 0.00000000e+00  3.78952978e-03 -2.96767152e+00  9.26153929e-03\n",
      "  1.24648671e+01 -6.95490864e-02 -2.39734940e+01  1.10383619e-01\n",
      "  2.14220328e+01 -5.35680562e-02 -7.19511517e+00], Beta GD Ridge: [ 0.00000000e+00  1.91892035e-04 -1.02262351e+00 -6.90115050e-03\n",
      "  1.21924335e+00  6.72252324e-03 -4.72602678e-02  5.79558509e-03\n",
      " -3.91830521e-01 -7.44612405e-03  1.65151350e-02]\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74362765e-03 -2.97425693e+00  9.68996101e-03\n",
      "  1.25135634e+01 -7.10135281e-02 -2.40997206e+01  1.12351339e-01\n",
      "  2.15578779e+01 -5.44655451e-02 -7.24695626e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891734e-04 -1.02262430e+00 -6.90113722e-03\n",
      "  1.21924665e+00  6.72253352e-03 -4.72643298e-02  5.79551506e-03\n",
      " -3.91829683e-01 -7.44607374e-03  1.65158504e-02]\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74359499e-03 -2.97426162e+00  9.69026586e-03\n",
      "  1.25135981e+01 -7.10145701e-02 -2.40998104e+01  1.12352740e-01\n",
      "  2.15579745e+01 -5.44661837e-02 -7.24699315e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891764e-04 -1.02262433e+00 -6.90113705e-03\n",
      "  1.21924681e+00  6.72253510e-03 -4.72645609e-02  5.79551079e-03\n",
      " -3.91829585e-01 -7.44607109e-03  1.65158574e-02]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuela/FYS-STK4155-group14/project_1/Code/functions.py:37: RuntimeWarning: overflow encountered in matmul\n",
      "  gradient = 2.0/n * (X.T @ (X @ theta) - X.T @ y) + 2*lam*theta\n",
      "/home/manuela/FYS-STK4155-group14/project_1/Code/functions.py:37: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = 2.0/n * (X.T @ (X @ theta) - X.T @ y) + 2*lam*theta\n",
      "/home/manuela/FYS-STK4155-group14/project_1/Code/functions.py:32: RuntimeWarning: overflow encountered in matmul\n",
      "  gradient = 2.0/n * (X.T @ X @ theta - X.T @ y)\n",
      "/home/manuela/FYS-STK4155-group14/project_1/Code/functions.py:32: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = 2.0/n * (X.T @ X @ theta - X.T @ y)\n",
      "/tmp/ipykernel_8971/3357555623.py:67: RuntimeWarning: invalid value encountered in subtract\n",
      "  beta_gd_o = beta_gd_o - lr * grad_OLS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.3\n",
      "MSE OLS: 0.001484546631866829, MSE Ridge: 0.0017395603039358032, MSE GD OLS: nan, MSE GD Ridge: nan\n",
      "R2 OLS: 0.9800049681741101, R2 Ridge: 0.976567517228484, R2 GD OLS: nan, R2 GD Ridge: nan\n",
      "Beta GD OLS: [ 0.00000000e+00  3.78952978e-03 -2.96767152e+00  9.26153929e-03\n",
      "  1.24648671e+01 -6.95490864e-02 -2.39734940e+01  1.10383619e-01\n",
      "  2.14220328e+01 -5.35680562e-02 -7.19511517e+00], Beta GD Ridge: [ 0.00000000e+00  1.91892035e-04 -1.02262351e+00 -6.90115050e-03\n",
      "  1.21924335e+00  6.72252324e-03 -4.72602678e-02  5.79558509e-03\n",
      " -3.91830521e-01 -7.44612405e-03  1.65151350e-02]\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74362765e-03 -2.97425693e+00  9.68996101e-03\n",
      "  1.25135634e+01 -7.10135281e-02 -2.40997206e+01  1.12351339e-01\n",
      "  2.15578779e+01 -5.44655451e-02 -7.24695626e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891734e-04 -1.02262430e+00 -6.90113722e-03\n",
      "  1.21924665e+00  6.72253352e-03 -4.72643298e-02  5.79551506e-03\n",
      " -3.91829683e-01 -7.44607374e-03  1.65158504e-02]\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74359499e-03 -2.97426162e+00  9.69026586e-03\n",
      "  1.25135981e+01 -7.10145701e-02 -2.40998104e+01  1.12352740e-01\n",
      "  2.15579745e+01 -5.44661837e-02 -7.24699315e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891764e-04 -1.02262433e+00 -6.90113705e-03\n",
      "  1.21924681e+00  6.72253510e-03 -4.72645609e-02  5.79551079e-03\n",
      " -3.91829585e-01 -7.44607109e-03  1.65158574e-02]\n",
      "Beta GD OLS: [nan nan nan nan nan nan nan nan nan nan nan], Beta GD Ridge: [nan nan nan nan nan nan nan nan nan nan nan]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Analysis of role of learning rate\n",
    "\n",
    "n = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "print('Ridge parameters:', beta_r)\n",
    "print('OLS parameters:', beta_o)\n",
    "y_ols = X_test_s @ beta_o + y_offset\n",
    "y_ridge = X_test_s @ beta_r + y_offset\n",
    "mse_ols = MSE(y_test, y_ols)\n",
    "mse_ridge = MSE(y_test, y_ridge)\n",
    "r2_ols = R2(y_test, y_ols)\n",
    "r2_ridge = R2(y_test, y_ridge)\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "mse_gd_ols = []\n",
    "mse_gd_ridge = []\n",
    "\n",
    "r2_gd_ols = []\n",
    "r2_gd_ridge = []\n",
    "\n",
    "beta_gd_ols = []\n",
    "beta_gd_ridge = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "\n",
    "    num_iters = 100000000\n",
    "\n",
    "    # Initialize weights for gradient descent\n",
    "    beta_gd_r = np.zeros(len(beta_r))\n",
    "    beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "    stopping_criteria = [1e-10]*len(beta_r)\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for t in range(num_iters):\n",
    "        # Compute gradients for Ridge\n",
    "        grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "        # Update parameters beta\n",
    "        beta_gd_r = beta_gd_r - lr * grad_Ridge\n",
    "        if (np.abs(- lr * grad_Ridge) < stopping_criteria).all():\n",
    "            print(\"Convergence reached at iteration for Ridge\", t)\n",
    "            break\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Compute gradients for OLS\n",
    "        grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "        # Update parameters beta\n",
    "        beta_gd_o = beta_gd_o - lr * grad_OLS\n",
    "        if (np.abs(- lr * grad_OLS) < stopping_criteria).all():\n",
    "            print(\"Convergence reached at iteration for OLS\", t)\n",
    "            break\n",
    "\n",
    "    beta_gd_ols.append(beta_gd_o)\n",
    "    beta_gd_ridge.append(beta_gd_r)\n",
    "\n",
    "    y_gd_ols = X_test_s @ beta_gd_ols[-1] + y_offset\n",
    "    y_gd_ridge = X_test_s @ beta_gd_ridge[-1] + y_offset\n",
    "\n",
    "    mse_gd_ols.append(MSE(y_test, y_gd_ols))\n",
    "    mse_gd_ridge.append(MSE(y_test, y_gd_ridge))\n",
    "\n",
    "    r2_gd_ols.append(R2(y_test, y_gd_ols))\n",
    "    r2_gd_ridge.append(R2(y_test, y_gd_ridge))\n",
    "\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"MSE OLS: {mse_ols}, MSE Ridge: {mse_ridge}, MSE GD OLS: {MSE(y_test, y_gd_ols)}, MSE GD Ridge: {MSE(y_test, y_gd_ridge)}\")\n",
    "    print(f\"R2 OLS: {r2_ols}, R2 Ridge: {r2_ridge}, R2 GD OLS: {R2(y_test, y_gd_ols)}, R2 GD Ridge: {R2(y_test, y_gd_ridge)}\")\n",
    "    for beta3, beta4 in zip(beta_gd_ols, beta_gd_ridge):\n",
    "        print(f\"Beta GD OLS: {beta3}, Beta GD Ridge: {beta4}\")\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1a542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS coefficients (scikit-learn): [0.         1.15848065]\n",
      "Ridge parameters: [0.         1.15847921]\n",
      "OLS parameters: [0.         1.15848065]\n",
      "Convergence reached at iteration for Ridge 8463\n",
      "Convergence reached at iteration for OLS 8471\n",
      "Final beta GD OLS: [0.        1.1584806]\n",
      "Final beta GD Ridge: [0.         1.15732328]\n",
      "Learning rate: 0.001\n",
      "MSE OLS: 0.9834027777777774, MSE Ridge: 0.9834028810783095, MSE GD OLS: 0.9834027813351598, MSE GD Ridge: 0.9834866472041199\n",
      "R2 OLS: 0.5712624740324754, R2 Ridge: 0.5712624483045504, R2 GD OLS: 0.5712624731464901, R2 GD Ridge: 0.5712413422155926\n",
      "Beta GD OLS: [0.        1.1584806], Beta GD Ridge: [0.         1.15732328]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Validation of everything\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import OLS_gradient, Ridge_gradient, polynomial_features\n",
    "\n",
    "#My gradient descent implementation\n",
    "\n",
    "n = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(-1,1, n)\n",
    "#y = runge(x) + 0.1*np.random.normal(0,1, x.shape)\n",
    "y = 1 + 2*x\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 1)\n",
    "X_test = polynomial_features(x_test, 1)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Scale data before fitting the OLS model\n",
    "scaler_x = StandardScaler()\n",
    "X_train_s = scaler_x.fit_transform(X_train)\n",
    "ols_model = LinearRegression(fit_intercept=False)\n",
    "ols_model.fit(X_train_s, y_train)\n",
    "print(\"OLS coefficients (scikit-learn):\", ols_model.coef_)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "print('Ridge parameters:', beta_r)\n",
    "print('OLS parameters:', beta_o)\n",
    "y_ols = X_test_s @ beta_o\n",
    "y_ridge = X_test_s @ beta_r\n",
    "mse_ols = MSE(y_test, y_ols)\n",
    "mse_ridge = MSE(y_test, y_ridge)\n",
    "r2_ols = R2(y_test, y_ols)\n",
    "r2_ridge = R2(y_test, y_ridge)\n",
    "\n",
    "learning_rates = [0.001] #, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "mse_gd_ols = []\n",
    "mse_gd_ridge = []\n",
    "\n",
    "r2_gd_ols = []\n",
    "r2_gd_ridge = []\n",
    "\n",
    "beta_gd_ols = []\n",
    "beta_gd_ridge = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "\n",
    "    num_iters = 100000000\n",
    "\n",
    "    # Initialize weights for gradient descent\n",
    "    beta_gd_r = np.zeros(len(beta_r))\n",
    "    beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "    stopping_criteria = [1e-10]*len(beta_r)\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for t in range(num_iters):\n",
    "        # Compute gradients for Ridge\n",
    "        grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "        # Update parameters beta\n",
    "        beta_gd_r = beta_gd_r - lr * grad_Ridge\n",
    "        if (np.abs(- lr * grad_Ridge) < stopping_criteria).all():\n",
    "            print(\"Convergence reached at iteration for Ridge\", t)\n",
    "            break\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Compute gradients for OLS\n",
    "        grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "        # Update parameters beta\n",
    "        beta_gd_o = beta_gd_o - lr * grad_OLS\n",
    "        if (np.abs(- lr * grad_OLS) < stopping_criteria).all():\n",
    "            print(\"Convergence reached at iteration for OLS\", t)\n",
    "            break\n",
    "\n",
    "    print(\"Final beta GD OLS:\", beta_gd_o)\n",
    "    print(\"Final beta GD Ridge:\", beta_gd_r)\n",
    "    beta_gd_ols.append(beta_gd_o)\n",
    "    beta_gd_ridge.append(beta_gd_r)\n",
    "\n",
    "    y_gd_ols = X_test_s @ beta_gd_ols[-1]\n",
    "    y_gd_ridge = X_test_s @ beta_gd_ridge[-1]\n",
    "\n",
    "    mse_gd_ols.append(MSE(y_test, y_gd_ols))\n",
    "    mse_gd_ridge.append(MSE(y_test, y_gd_ridge))\n",
    "\n",
    "    r2_gd_ols.append(R2(y_test, y_gd_ols))\n",
    "    r2_gd_ridge.append(R2(y_test, y_gd_ridge))\n",
    "\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"MSE OLS: {mse_ols}, MSE Ridge: {mse_ridge}, MSE GD OLS: {MSE(y_test, y_gd_ols)}, MSE GD Ridge: {MSE(y_test, y_gd_ridge)}\")\n",
    "    print(f\"R2 OLS: {r2_ols}, R2 Ridge: {r2_ridge}, R2 GD OLS: {R2(y_test, y_gd_ols)}, R2 GD Ridge: {R2(y_test, y_gd_ridge)}\")\n",
    "    for beta3, beta4 in zip(beta_gd_ols, beta_gd_ridge):\n",
    "        print(f\"Beta GD OLS: {beta3}, Beta GD Ridge: {beta4}\")\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
