{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe8f83c",
   "metadata": {},
   "source": [
    "# Kodekladd\n",
    "\n",
    "According to Week 36:\n",
    "\n",
    "OLS:\n",
    "$$\n",
    "\\nabla_{\\theta} C(\\theta) = \\frac{2}{n}X^T(X\\theta - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "Ridge:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+2\\lambda \\theta\n",
    "$$\n",
    "\n",
    "Lasso:\n",
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+\\lambda sgn(\\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b507c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import OLS_gradient, Ridge_gradient, polynomial_features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9a34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "np.random.seed(42)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Hessian matrix\n",
    "H_ridge = (2.0/len(X_train_s))* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues_ridge, EigVectors_ridge = np.linalg.eig(H_ridge)\n",
    "\n",
    "H_OLS = (2.0/len(X_train_s))* X_train_s.T @ X_train_s \n",
    "EigValues_OLS, EigVectors_OLS = np.linalg.eig(H_OLS)\n",
    "\n",
    "np.savez('data_arrays_last.npz', X_train_s=X_train_s, X_test_s=X_test_s, y_test=y_test, y_train=y_train, y_offset=y_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n",
      "Convergence reached at iteration for Ridge 33485\n",
      "Convergence reached at iteration for OLS 12672432\n",
      "Learning rate: 0.12787918344577412\n",
      "MSE GD OLS: 0.0014845451758879675, MSE GD Ridge: 0.00975592438969502\n",
      "R2 GD OLS: 0.9800049877260462, R2 GD Ridge: 0.8688669087299332\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74361341e-03 -2.97425898e+00  9.69009393e-03\n",
      "  1.25135785e+01 -7.10139824e-02 -2.40997597e+01  1.12351950e-01\n",
      "  2.15579200e+01 -5.44658235e-02 -7.24697234e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891745e-04 -1.02262431e+00 -6.90113712e-03\n",
      "  1.21924672e+00  6.72253417e-03 -4.72644304e-02  5.79551322e-03\n",
      " -3.91829642e-01 -7.44607258e-03  1.65158549e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Addition of momentum\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_ridge)\n",
    "num_iters = 100000000\n",
    "momentum = 0.3\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "change = 0.0\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    # Calculate change with momentum\n",
    "    new_change = lr * grad_Ridge + momentum * change\n",
    "    # Update parameters beta\n",
    "    beta_gd_r = beta_gd_r - new_change\n",
    "    # Save change for next iteration\n",
    "    change = new_change\n",
    "    # Check for convergence\n",
    "    if (np.abs(new_change) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_OLS)\n",
    "change = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    # Calculate change with momentum\n",
    "    new_change = lr * grad_OLS + momentum * change\n",
    "    # Update parameters beta\n",
    "    beta_gd_o = beta_gd_o - new_change\n",
    "    # Save change for next iteration\n",
    "    change = new_change\n",
    "    # Check for convergence\n",
    "    if (np.abs(new_change) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_momentum = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('momentum_results.json', 'w') as f:\n",
    "    json.dump(dict_momentum, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c28de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n",
      "Convergence reached at iteration for Ridge 50479\n",
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "Convergence reached at iteration for OLS 20125984\n",
      "Learning rate: 0.12791189798390853\n",
      "MSE GD OLS: 0.0014845451832575852, MSE GD Ridge: 0.009755924657122474\n",
      "R2 GD OLS: 0.9800049876269525, R2 GD Ridge: 0.8688669051486893\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74361625e-03 -2.97425901e+00  9.69006813e-03\n",
      "  1.25135788e+01 -7.10139059e-02 -2.40997604e+01  1.12351860e-01\n",
      "  2.15579208e+01 -5.44657868e-02 -7.24697262e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891916e-04 -1.02262430e+00 -6.90113739e-03\n",
      "  1.21924667e+00  6.72253030e-03 -4.72644483e-02  5.79552228e-03\n",
      " -3.91829511e-01 -7.44607781e-03  1.65157713e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ADAgrad\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_ridge)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "G_iter = 0.0\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    G_iter += grad_Ridge*grad_Ridge\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_Ridge\n",
    "    beta_gd_r = beta_gd_r - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_OLS)\n",
    "G_iter = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    G_iter += grad_OLS*grad_OLS\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_OLS\n",
    "    beta_gd_o = beta_gd_o - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_adagrad = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('adagrad_results.json', 'w') as f:\n",
    "    json.dump(dict_adagrad, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a09fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.10232951838712687\n",
      "MSE GD OLS: 0.0931552876294208, MSE GD Ridge: 0.10201309005678538\n",
      "R2 GD OLS: -0.24278558954982699, R2 GD Ridge: -0.37209131819408325\n",
      "Beta GD OLS: [  0.          -0.0474212   -2.92310155  -0.04147419  12.5647975\n",
      "  -0.12218037 -24.04873546   0.06118938  21.60923597  -0.10563158\n",
      "  -7.19586528], Beta GD Ridge: [ 0.         -0.0509624  -0.97147008 -0.05805543  1.27040125 -0.04443175\n",
      "  0.00388949 -0.04535878 -0.34067516 -0.05860036  0.06767013]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#RMSprop\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_ridge)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "G_iter = 0.0\n",
    "rho = 0.9\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    G_iter = rho*G_iter + (1-rho)*grad_Ridge*grad_Ridge\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_Ridge\n",
    "    beta_gd_r = beta_gd_r - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_OLS)\n",
    "G_iter = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    G_iter = (rho*G_iter + (1-rho)*grad_OLS*grad_OLS)\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(G_iter) + delta)) * grad_OLS\n",
    "    beta_gd_o = beta_gd_o - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_rmsprop = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('rmsprop_results.json', 'w') as f:\n",
    "    json.dump(dict_rmsprop, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91fcb8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached at iteration for Ridge 20579220\n",
      "Learning rate: 0.10232951838712687\n",
      "MSE GD OLS: 0.0014949855074251352, MSE GD Ridge: 0.009755557664900051\n",
      "R2 GD OLS: 0.9798638841650656, R2 GD Ridge: 0.8688723565479857\n",
      "Beta GD OLS: [ 0.00000000e+00  4.33871766e-03 -2.97486146e+00  1.02857261e-02\n",
      "  1.25130376e+01 -7.04204568e-02 -2.41004954e+01  1.12949295e-01\n",
      "  2.15574761e+01 -5.38716670e-02 -7.24762519e+00], Beta GD Ridge: [ 0.00000000e+00  2.16998659e-04 -1.02264947e+00 -6.87603043e-03\n",
      "  1.21922185e+00  6.74764417e-03 -4.72899023e-02  5.82061314e-03\n",
      " -3.91854561e-01 -7.42096174e-03  1.64907360e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ADAM\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_ridge)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho_1 = 0.9\n",
    "rho_2 = 0.99\n",
    "first_moment = 0.0\n",
    "second_moment = 0.0\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    t += 1\n",
    "    # Compute gradients for Ridge\n",
    "    grad_Ridge = Ridge_gradient(X_train_s, y_train, beta_gd_r, lmbda)\n",
    "    # Computing moments first\n",
    "    first_moment = rho_1 * first_moment + (1 - rho_1) * grad_Ridge\n",
    "    second_moment = rho_2 * second_moment + (1 - rho_2) * grad_Ridge * grad_Ridge\n",
    "    first_term = first_moment / (1 - rho_1**(t))\n",
    "    second_term = second_moment / (1 - rho_2**(t))\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "    beta_gd_r = beta_gd_r - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for Ridge\", t)\n",
    "        break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_OLS)\n",
    "first_moment = 0.0\n",
    "second_moment = 0.0\n",
    "\n",
    "for t in range(num_iters):\n",
    "    t += 1\n",
    "    # Compute gradients for OLS\n",
    "    grad_OLS = OLS_gradient(X_train_s, y_train, beta_gd_o)\n",
    "    # Computing moments first\n",
    "    first_moment = rho_1 * first_moment + (1 - rho_1) * grad_OLS\n",
    "    second_moment = rho_2 * second_moment + (1 - rho_2) * grad_OLS * grad_OLS\n",
    "    first_term = first_moment / (1 - rho_1**(t))\n",
    "    second_term = second_moment / (1 - rho_2**(t))\n",
    "    # Update parameters beta\n",
    "    update = (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "    beta_gd_o = beta_gd_o - update\n",
    "    # Check for convergence\n",
    "    if (np.abs(update) < stopping_criteria).all():\n",
    "        print(\"Convergence reached at iteration for OLS\", t)\n",
    "        break\n",
    "\n",
    "y_gd_ols = X_test_s @ beta_gd_o + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_r + y_offset\n",
    "\n",
    "mse_gd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_gd_ridge = MSE(y_test, y_gd_ridge)\n",
    "r2_gd_ols = R2(y_test, y_gd_ols)\n",
    "r2_gd_ridge = R2(y_test, y_gd_ridge)\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"MSE GD OLS: {mse_gd_ols}, MSE GD Ridge: {mse_gd_ridge}\")\n",
    "print(f\"R2 GD OLS: {r2_gd_ols}, R2 GD Ridge: {r2_gd_ridge}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_o}, Beta GD Ridge: {beta_gd_r}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_adam = {'Beta Ridge': beta_gd_r, 'Beta OLS': beta_gd_o, 'R2 Ridge': r2_gd_ridge, 'R2 OLS': r2_gd_ols,\n",
    "                 'MSE Ridge': mse_gd_ridge, 'MSE OLS': mse_gd_ols}\n",
    "with open('adam_results.json', 'w') as f:\n",
    "    json.dump(dict_adam, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
