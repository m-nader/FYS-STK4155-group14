{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b507c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import OLS_gradient, Ridge_gradient, Lasso_gradient, polynomial_features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4514875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data arrays\n",
    "data = np.load('data_arrays_last.npz')\n",
    "X_train_s = data['X_train_s']\n",
    "X_test_s = data['X_test_s']\n",
    "y_test = data['y_test']\n",
    "y_train = data['y_train']\n",
    "y_offset = data['y_offset']\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Hessian matrix\n",
    "H_ridge = (2.0/len(X_train_s))* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues_ridge, EigVectors_ridge = np.linalg.eig(H_ridge)\n",
    "\n",
    "H_OLS = (2.0/len(X_train_s))* X_train_s.T @ X_train_s \n",
    "EigValues_OLS, EigVectors_OLS = np.linalg.eig(H_OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2d72cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m stopping_criteria = [\u001b[32m1e-10\u001b[39m]*\u001b[38;5;28mlen\u001b[39m(beta_r)\n\u001b[32m     11\u001b[39m M = \u001b[32m100\u001b[39m   \u001b[38;5;66;03m#size of each minibatch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m m = \u001b[38;5;28mint\u001b[39m(\u001b[43mn\u001b[49m*\u001b[32m0.8\u001b[39m/M) \u001b[38;5;66;03m#number of minibatches\u001b[39;00m\n\u001b[32m     13\u001b[39m t0, t1 = \u001b[32m5\u001b[39m, \u001b[32m50\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearning_schedule\u001b[39m(t):\n",
      "\u001b[31mNameError\u001b[39m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "# Stochastic GD\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "n_epochs = 1000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "M = 100   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "# Stochastic Gradient Descent for Lasso Regression\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_lasso = beta_gd_lasso - eta*gradients\n",
    "\n",
    "# Stochastic Gradient Descent for OLS\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ols = beta_gd_ols - eta*gradients\n",
    "\n",
    "# Stochastic Gradient Descent for Ridge Regression\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ridge = beta_gd_ridge - eta*gradients\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE GD Lasso: 0.010587090019651826, MSE GD OLS: 0.010521229260240844, MSE GD Ridge: 0.010632194203567424\n",
      "R2 GD Lasso: 0.8594417592689789, R2 GD OLS: 0.8603198173781904, R2 GD Ridge: 0.8588434765844091\n",
      "Beta GD Lasso: [ 0.00000000e+00  7.77174568e-07 -8.20974252e-01  8.12355502e-07\n",
      "  6.10291796e-01  6.61165737e-07  3.11682329e-01  3.25102918e-07\n",
      " -2.58790718e-02 -6.21360925e-04 -2.93009177e-01]\n",
      "Beta GD OLS: [ 0.00000000e+00  4.58533312e-05 -8.55973807e-01  7.11011068e-04\n",
      "  6.43920257e-01  4.76451874e-04  3.58535588e-01 -1.08588305e-03\n",
      " -3.94304675e-02 -1.48149455e-03 -3.28863654e-01]\n",
      "Beta GD Ridge: [ 0.00000000e+00  4.59699349e-04 -8.19418910e-01 -9.89611168e-04\n",
      "  5.95604009e-01  2.11941815e-03  3.39449495e-01 -2.92580740e-04\n",
      " -3.04964114e-02 -2.40764139e-03 -3.03577085e-01]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Addition of momentum\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "momentum = 0.3\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "change = 0.0\n",
    "n_epochs = 1000000\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        # take a step\n",
    "        beta_gd_lasso -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        \n",
    "\n",
    "change = 0.0\n",
    "# Gradient descent loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        # take a step\n",
    "        beta_gd_ridge -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\n",
    "change = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        # take a step\n",
    "        beta_gd_ols -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_momentum = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_momentum_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_momentum, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c28de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE GD Lasso: 0.008732403716334425, MSE GD OLS: 0.010255434443650256, MSE GD Ridge: 0.011300545287658576\n",
      "R2 GD Lasso: 0.8840608825903667, R2 GD OLS: 0.8642256172898259, R2 GD Ridge: 0.849973816769186\n",
      "Beta GD Lasso: [ 0.          0.02371808 -1.16549808 -0.0896047   1.83630366  0.05179959\n",
      " -0.79854751  0.01435301 -0.29385334  0.00786607  0.16386655]\n",
      "Beta GD OLS: [  0.           0.10143781  -3.2685619   -1.08983918  15.50101467\n",
      "   3.4228095  -32.12097803  -4.40167068  30.37770345   1.91338992\n",
      " -10.73740189]\n",
      "Beta GD Ridge: [ 0.          0.0359991  -0.99097522  0.03272938  1.07446492 -0.13860456\n",
      " -0.00418639 -0.05314808 -0.16670444  0.15665191 -0.1703635 ]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ADAgrad\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_ridge)\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "n_epochs = 1000000\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        beta_gd_ridge -= update\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_OLS)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        beta_gd_lasso -= update\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        beta_gd_ols -= update\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_adagrad = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_adagrad_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_adagrad, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a09fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE GD Lasso: 0.04134056586779405, MSE GD OLS: 0.43819229748147154, MSE GD Ridge: 0.056230677949028626\n",
      "R2 GD Lasso: 0.447693749420948, R2 GD OLS: -4.7162103249144325, R2 GD Ridge: 0.24253545535699939\n",
      "Beta GD Lasso: [ 0.          0.03633336 -1.19801708  0.04833248  1.89385325  0.03536947\n",
      " -0.98411937  0.02615496 -0.10820222  0.02127051  0.02720054]\n",
      "Beta GD OLS: [  0.           0.04811783  -2.99799412   0.06328426  12.31516304\n",
      "   0.06438068 -24.19965365   0.19998768  21.39493408   0.12775397\n",
      "  -7.37872624]\n",
      "Beta GD Ridge: [ 0.         -0.05990633 -0.95697317 -0.06029727  1.2547493  -0.0261139\n",
      " -0.01948476 -0.00305751 -0.3596999   0.00940338  0.05647358]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#RMSprop\n",
    "\n",
    "#WHAT IS WRONG HERE?\n",
    "\n",
    "# There are a few issues in the RMSprop code:\n",
    "\n",
    "# 1. Giter should be initialized as a vector (np.zeros) with the same shape as beta, not as a scalar 0.0.\n",
    "# 2. You should use a separate Giter for each optimizer (Ridge, Lasso, OLS), not reuse the same variable.\n",
    "# 3. The learning rate (lr) should be set before each optimizer loop, as you do, but you should not reuse the same Giter variable.\n",
    "# 4. Remove the unused G_iter = 0.0 line.\n",
    "# 5. The update formula is correct, but make sure Giter is a vector.\n",
    "\n",
    "# Here is the corrected code:\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues_ridge)\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho = 0.99\n",
    "n_epochs = 1000000\n",
    "M = 50   # size of each minibatch\n",
    "m = int(len(X_train_s)/M) # number of minibatches\n",
    "\n",
    "# RMSprop for Ridge\n",
    "Giter_ridge = np.zeros(len(beta_r))\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        Giter_ridge = rho * Giter_ridge + (1 - rho) * gradients**2\n",
    "        update = lr * gradients / (delta + np.sqrt(Giter_ridge))\n",
    "        beta_gd_ridge -= update\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\n",
    "# RMSprop for Lasso\n",
    "lr = 1.0 / np.max(EigValues_OLS)\n",
    "Giter_lasso = np.zeros(len(beta_r))\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        Giter_lasso = rho * Giter_lasso + (1 - rho) * gradients**2\n",
    "        update = lr * gradients / (delta + np.sqrt(Giter_lasso))\n",
    "        beta_gd_lasso -= update\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\n",
    "# RMSprop for OLS\n",
    "Giter_ols = np.zeros(len(beta_r))\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        Giter_ols = rho * Giter_ols + (1 - rho) * gradients**2\n",
    "        update = lr * gradients / (delta + np.sqrt(Giter_ols))\n",
    "        beta_gd_ols -= update\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break    \n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_rmsprop = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_rmsprop_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_rmsprop, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcb8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE GD Lasso: 0.11852301333784303, MSE GD OLS: 0.012081920512245794, MSE GD Ridge: 0.36413125928611406\n",
      "R2 GD Lasso: -0.5602829348121219, R2 GD OLS: 0.8395942218334782, R2 GD Ridge: -3.7975577192891343\n",
      "Beta GD Lasso: [ 0.          0.0545191  -1.28518425  0.02268624  1.92513844  0.00818364\n",
      " -0.91115743  0.02165781 -0.10878883  0.05234437 -0.12757656]\n",
      "Beta GD OLS: [ 0.00000000e+00 -4.42246027e-03 -2.93663548e+00  2.19981897e-01\n",
      "  1.22593123e+01 -2.16768232e-01 -2.37399061e+01  1.60263990e-01\n",
      "  2.14396164e+01 -6.57634322e-02 -7.25623321e+00]\n",
      "Beta GD Ridge: [ 0.          0.05390852 -1.04992198  0.22694852  1.24049142 -0.04981653\n",
      "  0.060943    0.09186162 -0.26677001  0.22493202  0.01778813]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ADAM\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters for Ridge\n",
    "lr = 1.0 / np.max(EigValues_ridge)\n",
    "stopping_criteria = 1e-10  # Using a scalar\n",
    "delta = 1e-8\n",
    "rho_1 = 0.9\n",
    "rho_2 = 0.99\n",
    "n_epochs = 1000000\n",
    "M = 50   # size of each minibatch\n",
    "m = int(len(X_train_s)/M) # number of minibatches\n",
    "iter = 0\n",
    "\n",
    "# Initialize moments only once\n",
    "first_moment_ridge = np.zeros(len(beta_r))\n",
    "second_moment_ridge = np.zeros(len(beta_r))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(0, n - M)  # Number of samples should be adjusted\n",
    "        xi = X_train_s[random_index:random_index + M]\n",
    "        yi = y_train[random_index:random_index + M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        \n",
    "        # Updating moments\n",
    "        first_moment_ridge = rho_1 * first_moment_ridge + (1 - rho_1) * gradients\n",
    "        second_moment_ridge = rho_2 * second_moment_ridge + (1 - rho_2) * gradients * gradients\n",
    "        \n",
    "        # Bias-corrected moments\n",
    "        first_term = first_moment_ridge / (1 - rho_1 ** (iter))\n",
    "        second_term = second_moment_ridge / (1 - rho_2 ** (iter))\n",
    "\n",
    "        # Update parameters beta\n",
    "        update = (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "        beta_gd_ridge -= update\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(update) < stopping_criteria:\n",
    "            print(\"Convergence reached for Ridge at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "# Repeat the similar structure for Lasso and OLS\n",
    "\n",
    "# For Lasso\n",
    "lr = 1.0 / np.max(EigValues_OLS)\n",
    "# Initialize moments\n",
    "first_moment_lasso = np.zeros(len(beta_r))\n",
    "second_moment_lasso = np.zeros(len(beta_r))\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(0, n - M)\n",
    "        xi = X_train_s[random_index:random_index + M]\n",
    "        yi = y_train[random_index:random_index + M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        \n",
    "        # Updating moments\n",
    "        first_moment_lasso = rho_1 * first_moment_lasso + (1 - rho_1) * gradients\n",
    "        second_moment_lasso = rho_2 * second_moment_lasso + (1 - rho_2) * gradients * gradients\n",
    "\n",
    "        first_term = first_moment_lasso / (1 - rho_1 ** (iter))\n",
    "        second_term = second_moment_lasso / (1 - rho_2 ** (iter))\n",
    "\n",
    "        # Update parameters beta\n",
    "        update = (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "        beta_gd_lasso -= update\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(update) < stopping_criteria:\n",
    "            print(\"Convergence reached for Lasso at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "# For OLS\n",
    "# Initialize moments\n",
    "first_moment_ols = np.zeros(len(beta_r))\n",
    "second_moment_ols = np.zeros(len(beta_r))\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    iter = 0\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(0, n - M)\n",
    "        xi = X_train_s[random_index:random_index + M]\n",
    "        yi = y_train[random_index:random_index + M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        \n",
    "        # Updating moments\n",
    "        first_moment_ols = rho_1 * first_moment_ols + (1 - rho_1) * gradients\n",
    "        second_moment_ols = rho_2 * second_moment_ols + (1 - rho_2) * gradients * gradients\n",
    "        \n",
    "        first_term = first_moment_ols / (1 - rho_1 ** (iter))\n",
    "        second_term = second_moment_ols / (1 - rho_2 ** (iter))\n",
    "\n",
    "        # Update parameters beta\n",
    "        update = (lr / (np.sqrt(second_term) + delta)) * first_term\n",
    "        beta_gd_ols -= update\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(update) < stopping_criteria:\n",
    "            print(\"Convergence reached for OLS at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "# Prediction and evaluation\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "# Print results\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_adam = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                  'R2 GD Lasso': r2_sgd_lasso,\n",
    "                  'Beta GD Lasso': beta_gd_lasso,\n",
    "                  'MSE GD OLS': mse_sgd_ols,\n",
    "                  'R2 GD OLS': r2_sgd_ols,\n",
    "                  'Beta GD OLS': beta_gd_ols,\n",
    "                  'MSE GD Ridge': mse_sgd_ridge,\n",
    "                  'R2 GD Ridge': r2_sgd_ridge,\n",
    "                  'Beta GD Ridge': beta_gd_ridge}\n",
    "\n",
    "# Save results to JSON\n",
    "with open('sgd_adam_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_adam, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
