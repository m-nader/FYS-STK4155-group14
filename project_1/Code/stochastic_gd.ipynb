{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b507c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import OLS_gradient, Ridge_gradient, Lasso_gradient, polynomial_features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic GD\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "n_epochs = 50000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "# Stochastic Gradient Descent for Lasso Regression\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_lasso = beta_gd_lasso - eta*gradients\n",
    "\n",
    "# Stochastic Gradient Descent for OLS\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ols = beta_gd_ols - eta*gradients\n",
    "\n",
    "# Stochastic Gradient Descent for Ridge Regression\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ridge = beta_gd_ridge - eta*gradients\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge parameters: [ 0.00000000e+00  5.54300134e-03 -2.59074814e+00 -7.44146292e-03\n",
      "  9.69505866e+00 -9.50339784e-03 -1.68243883e+01  2.65540645e-02\n",
      "  1.37520890e+01 -1.42341245e-02 -4.27508408e+00]\n",
      "OLS parameters: [ 0.00000000e+00  3.74356233e-03 -2.97426630e+00  9.69057072e-03\n",
      "  1.25136327e+01 -7.10156122e-02 -2.40999002e+01  1.12354140e-01\n",
      "  2.15580712e+01 -5.44668224e-02 -7.24703004e+00]\n",
      "Convergence reached at iteration for Ridge 22131\n",
      "Convergence reached at iteration for OLS 8405293\n",
      "Learning rate: 0.2\n",
      "MSE OLS: 0.001484546631866829, MSE Ridge: 0.0017395603039358032, MSE GD OLS: 0.0014845457008599405, MSE GD Ridge: 0.009755924145600206\n",
      "R2 OLS: 0.9800049681741101, R2 Ridge: 0.976567517228484, R2 GD OLS: 0.9800049806763442, R2 GD Ridge: 0.8688669119993857\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74359499e-03 -2.97426162e+00  9.69026586e-03\n",
      "  1.25135981e+01 -7.10145701e-02 -2.40998104e+01  1.12352739e-01\n",
      "  2.15579745e+01 -5.44661837e-02 -7.24699315e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891764e-04 -1.02262433e+00 -6.90113705e-03\n",
      "  1.21924681e+00  6.72253510e-03 -4.72645610e-02  5.79551079e-03\n",
      " -3.91829585e-01 -7.44607109e-03  1.65158575e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Addition of momentum\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "num_iters = 100000000\n",
    "momentum = 0.3\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "change = 0.0\n",
    "n_epochs = 500\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        # take a step\n",
    "        beta_gd_lasso -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "\n",
    "change = 0.0\n",
    "# Gradient descent loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        # take a step\n",
    "        beta_gd_ridge -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "\n",
    "change = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        # take a step\n",
    "        beta_gd_ols -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_momentum = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_momentum_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_momentum, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c28de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge parameters: [ 0.00000000e+00  5.54300134e-03 -2.59074814e+00 -7.44146292e-03\n",
      "  9.69505866e+00 -9.50339784e-03 -1.68243883e+01  2.65540645e-02\n",
      "  1.37520890e+01 -1.42341245e-02 -4.27508408e+00]\n",
      "OLS parameters: [ 0.00000000e+00  3.74356233e-03 -2.97426630e+00  9.69057072e-03\n",
      "  1.25136327e+01 -7.10156122e-02 -2.40999002e+01  1.12354140e-01\n",
      "  2.15580712e+01 -5.44668224e-02 -7.24703004e+00]\n",
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n",
      "Convergence reached at iteration for Ridge 50479\n",
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "#ADAgrad\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "n_epochs = 500\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ridge -= update\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_lasso -= update\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ols -= update\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_adagrad = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_adagrad_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_adagrad, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a09fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge parameters: [ 0.00000000e+00  5.54300134e-03 -2.59074814e+00 -7.44146292e-03\n",
      "  9.69505866e+00 -9.50339784e-03 -1.68243883e+01  2.65540645e-02\n",
      "  1.37520890e+01 -1.42341245e-02 -4.27508408e+00]\n",
      "OLS parameters: [ 0.00000000e+00  3.74356233e-03 -2.97426630e+00  9.69057072e-03\n",
      "  1.25136327e+01 -7.10156122e-02 -2.40999002e+01  1.12354140e-01\n",
      "  2.15580712e+01 -5.44668224e-02 -7.24703004e+00]\n",
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n"
     ]
    }
   ],
   "source": [
    "#RMSprop\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho = 0.99\n",
    "n_epochs = 500\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\t# Hadamard product\n",
    "        beta_gd_ridge -= update\n",
    "\n",
    "# Hessian matrix for OLS\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\t# Hadamard product\n",
    "        beta_gd_lasso -= update\n",
    "\n",
    "G_iter = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\t# Hadamard product\n",
    "        beta_gd_ols -= update\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_rmsprop = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_rmsprop_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_rmsprop, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcb8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge parameters: [ 0.00000000e+00  5.54300134e-03 -2.59074814e+00 -7.44146292e-03\n",
      "  9.69505866e+00 -9.50339784e-03 -1.68243883e+01  2.65540645e-02\n",
      "  1.37520890e+01 -1.42341245e-02 -4.27508408e+00]\n",
      "OLS parameters: [ 0.00000000e+00  3.74356233e-03 -2.97426630e+00  9.69057072e-03\n",
      "  1.25136327e+01 -7.10156122e-02 -2.40999002e+01  1.12354140e-01\n",
      "  2.15580712e+01 -5.44668224e-02 -7.24703004e+00]\n",
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n",
      "Convergence reached at iteration for Ridge 2060\n",
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "Convergence reached at iteration for OLS 660108\n",
      "Learning rate: 0.12791189798390853\n",
      "MSE OLS: 0.10997884456748792, MSE Ridge: 0.10913833858639004, MSE GD OLS: 0.00148454516841589, MSE GD Ridge: 0.009755924020089264\n",
      "R2 OLS: 0.39816788476202536, R2 Ridge: 0.3991367745434813, R2 GD OLS: 0.9800049878265542, R2 GD Ridge: 0.8688669136814877\n",
      "Beta GD OLS: [ 0.00000000e+00  3.74361298e-03 -2.97425893e+00  9.69009572e-03\n",
      "  1.25135783e+01 -7.10139877e-02 -2.40997590e+01  1.12351955e-01\n",
      "  2.15579192e+01 -5.44658253e-02 -7.24697204e+00], Beta GD Ridge: [ 0.00000000e+00  1.91891936e-04 -1.02262434e+00 -6.90113769e-03\n",
      "  1.21924684e+00  6.72253748e-03 -4.72645908e-02  5.79550762e-03\n",
      " -3.91829600e-01 -7.44606952e-03  1.65158766e-02]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ADAM\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_r = np.zeros(len(beta_r))\n",
    "beta_gd_o = np.zeros(len(beta_o))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "num_iters = 100000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho_1 = 0.9\n",
    "rho_2 = 0.999\n",
    "n_epochs = 500\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        # Computing moments first\n",
    "        first_moment = rho_1*first_moment + (1-rho_1)*gradients\n",
    "        second_moment = rho_2*second_moment+(1-rho_2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-rho_1**iter)\n",
    "        second_term = second_moment/(1.0-rho_2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = lr*first_term/(np.sqrt(second_term)+delta)\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ridge -= update\n",
    "\n",
    "# Hessian matrix for OLS\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        # Computing moments first\n",
    "        first_moment = rho_1*first_moment + (1-rho_1)*gradients\n",
    "        second_moment = rho_2*second_moment+(1-rho_2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-rho_1**iter)\n",
    "        second_term = second_moment/(1.0-rho_2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = lr*first_term/(np.sqrt(second_term)+delta)\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_lasso -= update\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        # Computing moments first\n",
    "        first_moment = rho_1*first_moment + (1-rho_1)*gradients\n",
    "        second_moment = rho_2*second_moment+(1-rho_2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-rho_1**iter)\n",
    "        second_term = second_moment/(1.0-rho_2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = lr*first_term/(np.sqrt(second_term)+delta)\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ols -= update\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_adam = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_adam_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_adam, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
