{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b507c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import runge, MSE, R2, Ridge_parameters, OLS_parameters\n",
    "from functions import OLS_gradient, Ridge_gradient, Lasso_gradient, polynomial_features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2d72cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE GD Lasso: 0.013901044020342122, MSE GD OLS: 0.013470444942618677, MSE GD Ridge: 0.013828406386372775\n",
      "R2 GD Lasso: 0.8133188130154931, R2 GD OLS: 0.8191096404544187, R2 GD Ridge: 0.8143024904396763\n",
      "Beta GD Lasso: [ 0.00000000e+00 -1.02815588e-03 -7.22962345e-01  5.60234355e-07\n",
      "  4.61894282e-01  3.35114045e-06  2.92973095e-01  1.42769517e-05\n",
      " -8.74013097e-03  1.89483013e-05 -2.34684049e-01]\n",
      "Beta GD OLS: [ 0.00000000e+00 -5.25026009e-03 -7.49042314e-01  3.99678092e-03\n",
      "  4.87113854e-01  2.07421831e-03  3.20995979e-01 -6.00197867e-04\n",
      " -9.48468487e-04 -1.84911613e-03 -2.72835902e-01]\n",
      "Beta GD Ridge: [ 0.00000000e+00 -5.20385769e-03 -7.26724866e-01  3.10652380e-03\n",
      "  4.58602303e-01  2.79579197e-03  3.08431078e-01 -6.58993373e-05\n",
      "  4.14592351e-03 -2.21554616e-03 -2.56953559e-01]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Stochastic GD\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "n_epochs = 1000000\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "M = 100   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "# Stochastic Gradient Descent for Lasso Regression\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_lasso = beta_gd_lasso - eta*gradients\n",
    "\n",
    "# Stochastic Gradient Descent for OLS\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ols = beta_gd_ols - eta*gradients\n",
    "\n",
    "# Stochastic Gradient Descent for Ridge Regression\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        if np.linalg.norm(eta*gradients) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ridge = beta_gd_ridge - eta*gradients\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f4610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE GD Lasso: 0.01262056973799782, MSE GD OLS: 0.012285719326575469, MSE GD Ridge: 0.012625390738191997\n",
      "R2 GD Lasso: 0.8305095383403167, R2 GD OLS: 0.8350078144836346, R2 GD Ridge: 0.8304471649420873\n",
      "Beta GD Lasso: [ 0.00000000e+00 -1.22643179e-03 -8.06382727e-01 -1.70912445e-06\n",
      "  5.91605118e-01 -1.20755289e-06  3.07588732e-01 -1.01143392e-06\n",
      " -2.45148841e-02 -1.09056131e-06 -2.87338463e-01]\n",
      "Beta GD OLS: [ 0.00000000e+00 -3.66729107e-03 -8.40440780e-01  1.58353935e-03\n",
      "  6.25675248e-01  2.82022404e-03  3.51549380e-01 -3.47505264e-04\n",
      " -3.68625049e-02 -2.70222663e-03 -3.22930517e-01]\n",
      "Beta GD Ridge: [ 0.00000000e+00 -3.70976252e-03 -8.07108793e-01  8.37345077e-04\n",
      "  5.80712762e-01  3.65068169e-03  3.35079477e-01  5.90741696e-04\n",
      " -2.79617874e-02 -3.47308360e-03 -3.00482054e-01]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Addition of momentum\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Initialize hyperparameters\n",
    "momentum = 0.3\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "change = 0.0\n",
    "n_epochs = 1000000\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        # take a step\n",
    "        beta_gd_lasso -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "\n",
    "change = 0.0\n",
    "# Gradient descent loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        # take a step\n",
    "        beta_gd_ridge -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "\n",
    "change = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+momentum*change\n",
    "        if np.linalg.norm(new_change) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        # take a step\n",
    "        beta_gd_ols -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_momentum = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_momentum_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_momentum, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c28de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n",
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "MSE GD Lasso: 0.01888055341267186, MSE GD OLS: 0.004551946353780506, MSE GD Ridge: 0.028368686222391925\n",
      "R2 GD Lasso: 0.746437706350397, R2 GD OLS: 0.9386814008148571, R2 GD Ridge: 0.6207881576758889\n",
      "Beta GD Lasso: [ 0.00000000e+00 -9.31203724e-02 -1.15082557e+00  1.58078216e-01\n",
      "  1.59114472e+00 -1.59703338e-02 -6.21278762e-01 -4.90739595e-02\n",
      " -8.30962834e-02 -3.03997795e-05 -6.38269226e-02]\n",
      "Beta GD OLS: [  0.           0.20488812  -3.27780143  -1.27403624  15.418296\n",
      "   3.55199227 -32.09378228  -4.38894342  30.4265526    1.90156448\n",
      " -10.7442997 ]\n",
      "Beta GD Ridge: [ 0.          0.08484041 -1.04351666  0.07311213  1.30370814 -0.01513553\n",
      " -0.23743564 -0.22614943 -0.12854988  0.25167523 -0.15246093]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ADAgrad\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "n_epochs = 1000000\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ridge -= update\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_lasso -= update\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ols -= update\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_adagrad = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_adagrad_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_adagrad, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a09fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n",
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "MSE GD Lasso: 0.06700834447518494, MSE GD OLS: 0.11489694852781421, MSE GD Ridge: 0.16262036219586815\n",
      "R2 GD Lasso: 0.10181174696347595, R2 GD OLS: -0.5075180556184553, R2 GD Ridge: -1.1775154195999065\n",
      "Beta GD Lasso: [ 0.         -0.0610915  -3.29291909 -0.80159559  8.89824547  2.70997298\n",
      " -8.43857938 -3.88472191  0.88163617  2.18579371  1.58724708]\n",
      "Beta GD OLS: [  0.          -1.34002289  -4.89408271   4.32162118  19.79836162\n",
      "  -5.92651212 -33.41463376   5.13674268  25.52170907  -2.39690169\n",
      "  -7.30006864]\n",
      "Beta GD Ridge: [ 0.          0.11202038 -1.25201631  0.37869214  0.63543462 -0.84820825\n",
      " -0.051141    0.08818123 -0.22131068  0.37377582  0.69814807]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#RMSprop\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho = 0.99\n",
    "n_epochs = 1000000\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\t# Hadamard product\n",
    "        beta_gd_ridge -= update\n",
    "\n",
    "# Hessian matrix for OLS\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\t# Hadamard product\n",
    "        beta_gd_lasso -= update\n",
    "\n",
    "G_iter = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*lr/(delta+np.sqrt(Giter))\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "\t# Hadamard product\n",
    "        beta_gd_ols -= update\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_rmsprop = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_rmsprop_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_rmsprop, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fcb8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix:[7.81988102e+00 7.04651821e+00 6.45820952e-01 4.34618809e-01\n",
      " 4.34213818e-02 1.99862028e-02 3.31440644e-03 2.41897568e-03\n",
      " 2.00413804e-03 2.01589893e-03 2.00000000e-03]\n",
      "Eigenvalues of Hessian Matrix:[7.81788102e+00 7.04451821e+00 6.43820952e-01 4.32618809e-01\n",
      " 4.14213818e-02 1.79862028e-02 1.31440644e-03 4.18975683e-04\n",
      " 4.13804044e-06 1.58989315e-05 0.00000000e+00]\n",
      "MSE GD Lasso: 14.82022105350011, MSE GD OLS: 0.9371191468708926, MSE GD Ridge: 0.490705612375676\n",
      "R2 GD Lasso: -167.38507989381998, R2 GD OLS: -10.66638843371981, R2 GD Ridge: -5.3419023415426565\n",
      "Beta GD Lasso: [ 0.          0.47112312 -0.58796864  0.85083021  2.00191613  0.30718158\n",
      "  0.34418783  0.5639419   0.33894091  0.30154983  0.45454693]\n",
      "Beta GD OLS: [  0.           0.02370772  -2.8604561    0.19476483  11.66062018\n",
      "   0.74670853 -21.65356828  -0.58057181  18.84023102   0.65319818\n",
      "  -6.29624628]\n",
      "Beta GD Ridge: [ 0.         -0.07077638 -0.9023026   0.2370278   0.90256986  0.47475453\n",
      " -0.34303164 -0.29946771 -0.37455548  0.07137317 -0.16069099]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ADAM\n",
    "\n",
    "# Data generation\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-1,1, n)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1)\n",
    "\n",
    "# Split into training and test sets, scale data and create polynomial features\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = polynomial_features(x_train, 10)\n",
    "X_test = polynomial_features(x_test, 10)\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_offset = np.mean(y_train)\n",
    "\n",
    "lmbda = 0.001\n",
    "\n",
    "# Calculate parameters using OLS and Ridge closed form solutions\n",
    "beta_r = Ridge_parameters(X_train_s, y_train, lmbda)\n",
    "beta_o = OLS_parameters(X_train_s, y_train)\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "beta_gd_ols = np.zeros(len(beta_r))\n",
    "beta_gd_ridge = np.zeros(len(beta_r))\n",
    "beta_gd_lasso = np.zeros(len(beta_r))\n",
    "\n",
    "# Hessian matrix for Ridge\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s + 2*lmbda*np.eye(X_train_s.shape[1])\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "stopping_criteria = [1e-10]*len(beta_r)\n",
    "delta = 1e-8\n",
    "rho_1 = 0.9\n",
    "rho_2 = 0.999\n",
    "n_epochs = 1000000\n",
    "M = 50   #size of each minibatch\n",
    "m = int(n*0.8/M) #number of minibatches\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Ridge_gradient(xi, yi, beta_gd_ridge, lmbda)\n",
    "        # Computing moments first\n",
    "        first_moment = rho_1*first_moment + (1-rho_1)*gradients\n",
    "        second_moment = rho_2*second_moment+(1-rho_2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-rho_1**iter)\n",
    "        second_term = second_moment/(1.0-rho_2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = lr*first_term/(np.sqrt(second_term)+delta)\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ridge -= update\n",
    "\n",
    "# Hessian matrix for OLS\n",
    "H = (2.0/n)* X_train_s.T @ X_train_s \n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "# Initialize hyperparameters\n",
    "lr = 1.0 / np.max(EigValues)\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = Lasso_gradient(xi, yi, beta_gd_lasso, lmbda)\n",
    "        # Computing moments first\n",
    "        first_moment = rho_1*first_moment + (1-rho_1)*gradients\n",
    "        second_moment = rho_2*second_moment+(1-rho_2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-rho_1**iter)\n",
    "        second_term = second_moment/(1.0-rho_2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = lr*first_term/(np.sqrt(second_term)+delta)\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_lasso -= update\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X_train_s[random_index:random_index+M]\n",
    "        yi = y_train[random_index:random_index+M]\n",
    "        gradients = OLS_gradient(xi, yi, beta_gd_ols)\n",
    "        # Computing moments first\n",
    "        first_moment = rho_1*first_moment + (1-rho_1)*gradients\n",
    "        second_moment = rho_2*second_moment+(1-rho_2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-rho_1**iter)\n",
    "        second_term = second_moment/(1.0-rho_2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = lr*first_term/(np.sqrt(second_term)+delta)\n",
    "        if np.linalg.norm(update) < np.linalg.norm(stopping_criteria):\n",
    "            break\n",
    "        beta_gd_ols -= update\n",
    "\n",
    "y_gd_lasso = X_test_s @ beta_gd_lasso + y_offset\n",
    "y_gd_ols = X_test_s @ beta_gd_ols + y_offset\n",
    "y_gd_ridge = X_test_s @ beta_gd_ridge + y_offset\n",
    "\n",
    "mse_sgd_ols = MSE(y_test, y_gd_ols)\n",
    "mse_sgd_ridge = MSE(y_test, y_gd_ridge)\n",
    "mse_sgd_lasso = MSE(y_test, y_gd_lasso)\n",
    "r2_sgd_ols = R2(y_test, y_gd_ols)\n",
    "r2_sgd_ridge = R2(y_test, y_gd_ridge)\n",
    "r2_sgd_lasso = R2(y_test, y_gd_lasso)\n",
    "\n",
    "print(f\"MSE GD Lasso: {mse_sgd_lasso}, MSE GD OLS: {mse_sgd_ols}, MSE GD Ridge: {mse_sgd_ridge}\")\n",
    "print(f\"R2 GD Lasso: {r2_sgd_lasso}, R2 GD OLS: {r2_sgd_ols}, R2 GD Ridge: {r2_sgd_ridge}\")\n",
    "print(f\"Beta GD Lasso: {beta_gd_lasso}\")\n",
    "print(f\"Beta GD OLS: {beta_gd_ols}\")\n",
    "print(f\"Beta GD Ridge: {beta_gd_ridge}\")    \n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "dict_sgd_adam = {'MSE GD Lasso': mse_sgd_lasso,\n",
    "                        'R2 GD Lasso': r2_sgd_lasso,\n",
    "                        'Beta GD Lasso': beta_gd_lasso,\n",
    "                        'MSE GD OLS': mse_sgd_ols,\n",
    "                        'R2 GD OLS': r2_sgd_ols,\n",
    "                        'Beta GD OLS': beta_gd_ols,\n",
    "                        'MSE GD Ridge': mse_sgd_ridge,\n",
    "                        'R2 GD Ridge': r2_sgd_ridge,\n",
    "                        'Beta GD Ridge': beta_gd_ridge}\n",
    "with open('sgd_adam_results.json', 'w') as f:\n",
    "    json.dump(dict_sgd_adam, f, indent=4, default=lambda x: x.tolist() if hasattr(x, 'tolist') else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
