{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643ae874",
   "metadata": {},
   "source": [
    "# Neural network to approximate Runge's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0883116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import autograd.numpy as np  # We need to use this numpy wrapper to make automatic differentiation work later\n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import (\n",
    "    PolynomialFeatures,\n",
    ")\n",
    "import json\n",
    "\n",
    "from functions import runge, OLS_parameters, MSE\n",
    "from functions import ReLU, ReLU_der, sigmoid, sigmoid_der, softmax, softmax_vec, mse_der\n",
    "from functions import MSE, identity, identity_der, R2\n",
    "from nn_class import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52c19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "# data set from project 1\n",
    "x = np.linspace(-1,1, n).reshape(-1,1)\n",
    "y = runge(x) + 0.1*np.random.normal(0,1, x.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "y_offset = y_train.mean()\n",
    "\n",
    "poly = PolynomialFeatures(degree=10)\n",
    "X_train = poly.fit_transform(x_train)\n",
    "X_test = poly.fit_transform(x_test)\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f88ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 61967, cost change 4.82e-11 <= 1e-10\n",
      "NN mse (RMSProp): 0.02260516220854157\n",
      "NN R2 (RMSProp): 0.7620794216007972\n",
      "Early stopping at iteration 5558, cost change 8.66e-11 <= 1e-10\n",
      "NN mse (ADAM): 0.009370275269385123\n",
      "NN R2 (ADAM): 0.8858239098736672\n"
     ]
    }
   ],
   "source": [
    "# Two layer, 50 neurons, sigmoid activation function, plain gradient descent, rmsprop and adam\n",
    "layer_output_sizes = [50, 50, 1]\n",
    "activation_funcs = [sigmoid, sigmoid, identity]\n",
    "activation_ders = [sigmoid_der, sigmoid_der, identity_der]\n",
    "\n",
    "scaler.fit(x_train)\n",
    "x_train_s = scaler.transform(x_train)\n",
    "x_test_s = scaler.transform(x_test)\n",
    "inputs = x_train_s\n",
    "targets = y_train\n",
    "\n",
    "for option in ['RMSProp', 'ADAM']:\n",
    "    targets = y_train\n",
    "    NN = NeuralNetwork(\n",
    "        x_train_s,\n",
    "        targets,\n",
    "        layer_output_sizes,\n",
    "        activation_funcs,\n",
    "        activation_ders,\n",
    "        MSE,\n",
    "        mse_der,\n",
    "    )\n",
    "\n",
    "    NN.train_network_plain_gd(max_iter=1000000000, lr_method=option)\n",
    "\n",
    "    targets = y_test\n",
    "\n",
    "    predictions = NN.predict(x_test_s)\n",
    "    print(f'NN mse ({option}):', MSE(predictions, targets))\n",
    "    print(f'NN R2 ({option}):', R2(predictions, targets))\n",
    "\n",
    "    mse = MSE(predictions, targets)\n",
    "    r2 = R2(predictions, targets)\n",
    "\n",
    "    metrics = {\"mse\": float(mse), \"r2\": float(r2)}\n",
    "    with open(f\"nn_50_sigmoid_{option.lower()}_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a244c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 296093, cost change 1.56e-11 <= 1e-10\n",
      "NN mse (RMSProp): 0.02177745024279322\n",
      "NN R2 (RMSProp): 0.7689501348574588\n",
      "Early stopping at iteration 39300, cost change 6.20e-12 <= 1e-10\n",
      "NN mse (ADAM): 0.009564848875913155\n",
      "NN R2 (ADAM): 0.8834470025593197\n"
     ]
    }
   ],
   "source": [
    "# Two layer, 50 neurons, sigmoid activation function, plain gradient descent, rmsprop and adam, l2\n",
    "layer_output_sizes = [50, 50, 1]\n",
    "activation_funcs = [sigmoid, sigmoid, identity]\n",
    "activation_ders = [sigmoid_der, sigmoid_der, identity_der]\n",
    "\n",
    "scaler.fit(x_train)\n",
    "x_train_s = scaler.transform(x_train)\n",
    "x_test_s = scaler.transform(x_test)\n",
    "inputs = x_train_s\n",
    "targets = y_train\n",
    "\n",
    "for option in ['RMSProp', 'ADAM']:\n",
    "    targets = y_train\n",
    "    NN = NeuralNetwork(\n",
    "        x_train_s,\n",
    "        targets,\n",
    "        layer_output_sizes,\n",
    "        activation_funcs,\n",
    "        activation_ders,\n",
    "        MSE,\n",
    "        mse_der,\n",
    "        L2 = True,\n",
    "        lmbda = 0.001,\n",
    "    )\n",
    "\n",
    "    NN.train_network_plain_gd(max_iter=1000000000, lr_method=option)\n",
    "\n",
    "    targets = y_test\n",
    "\n",
    "    predictions = NN.predict(x_test_s)\n",
    "    print(f'NN mse ({option}):', MSE(predictions, targets))\n",
    "    print(f'NN R2 ({option}):', R2(predictions, targets))\n",
    "\n",
    "    mse = MSE(predictions, targets)\n",
    "    r2 = R2(predictions, targets)\n",
    "\n",
    "    metrics = {\"mse\": float(mse), \"r2\": float(r2)}\n",
    "    with open(f\"nn_50_sigmoid_{option.lower()}_l2_lmbda_0_001_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b0577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, cost: 0.034124\n",
      "Iteration 2000, cost: 0.030017\n",
      "Iteration 3000, cost: 0.029199\n",
      "Iteration 4000, cost: 0.028682\n",
      "Iteration 5000, cost: 0.028467\n",
      "Iteration 6000, cost: 0.028314\n",
      "Iteration 7000, cost: 0.028192\n",
      "Iteration 8000, cost: 0.028087\n",
      "Iteration 9000, cost: 0.027994\n",
      "Iteration 10000, cost: 0.027908\n",
      "Iteration 11000, cost: 0.027836\n",
      "Iteration 12000, cost: 0.027762\n",
      "Iteration 13000, cost: 0.027703\n",
      "Iteration 14000, cost: 0.027646\n",
      "Iteration 15000, cost: 0.027595\n",
      "Iteration 16000, cost: 0.027545\n",
      "Iteration 17000, cost: 0.026260\n",
      "Iteration 18000, cost: 0.026176\n",
      "Iteration 19000, cost: 0.026125\n",
      "Iteration 20000, cost: 0.026078\n",
      "Iteration 21000, cost: 0.026011\n",
      "Iteration 22000, cost: 0.024787\n",
      "Iteration 23000, cost: 0.024747\n",
      "Iteration 24000, cost: 0.024715\n",
      "Iteration 25000, cost: 0.024692\n",
      "Iteration 26000, cost: 0.024668\n",
      "Iteration 27000, cost: 0.024646\n",
      "Iteration 28000, cost: 0.024628\n",
      "Iteration 29000, cost: 0.024608\n",
      "Iteration 30000, cost: 0.024586\n",
      "Iteration 31000, cost: 0.024555\n",
      "Iteration 32000, cost: 0.024532\n",
      "Iteration 33000, cost: 0.024513\n",
      "Iteration 34000, cost: 0.024495\n",
      "Iteration 35000, cost: 0.024477\n",
      "Iteration 36000, cost: 0.024460\n",
      "Iteration 37000, cost: 0.024441\n",
      "Iteration 38000, cost: 0.024430\n",
      "Iteration 39000, cost: 0.024419\n",
      "Iteration 40000, cost: 0.024408\n",
      "Iteration 41000, cost: 0.024397\n",
      "Iteration 42000, cost: 0.024386\n",
      "Iteration 43000, cost: 0.024373\n",
      "Iteration 44000, cost: 0.024365\n",
      "Iteration 45000, cost: 0.024357\n",
      "Iteration 46000, cost: 0.024348\n",
      "Iteration 47000, cost: 0.024340\n",
      "Iteration 48000, cost: 0.024332\n",
      "Iteration 49000, cost: 0.024324\n",
      "Iteration 50000, cost: 0.024316\n",
      "Iteration 51000, cost: 0.024307\n",
      "Iteration 52000, cost: 0.024298\n",
      "Iteration 53000, cost: 0.024290\n",
      "Iteration 54000, cost: 0.024282\n",
      "Iteration 55000, cost: 0.024274\n",
      "Iteration 56000, cost: 0.024266\n",
      "Iteration 57000, cost: 0.024258\n",
      "Iteration 58000, cost: 0.024249\n",
      "Iteration 59000, cost: 0.024241\n",
      "Iteration 60000, cost: 0.024232\n",
      "Iteration 61000, cost: 0.024222\n",
      "Iteration 62000, cost: 0.024208\n",
      "Iteration 63000, cost: 0.024191\n",
      "Iteration 64000, cost: 0.024169\n",
      "Iteration 65000, cost: 0.024122\n",
      "Iteration 66000, cost: 0.024013\n",
      "Iteration 67000, cost: 0.023914\n",
      "Iteration 68000, cost: 0.023862\n",
      "Iteration 69000, cost: 0.023837\n",
      "Iteration 70000, cost: 0.023820\n",
      "Iteration 71000, cost: 0.023809\n",
      "Iteration 72000, cost: 0.023800\n",
      "Iteration 73000, cost: 0.023789\n",
      "Iteration 74000, cost: 0.023776\n",
      "Iteration 75000, cost: 0.023762\n",
      "Iteration 76000, cost: 0.023755\n",
      "Iteration 77000, cost: 0.023749\n",
      "Iteration 78000, cost: 0.023744\n",
      "Iteration 79000, cost: 0.023741\n",
      "Iteration 80000, cost: 0.023736\n",
      "Iteration 81000, cost: 0.023732\n",
      "Iteration 82000, cost: 0.023730\n",
      "Iteration 83000, cost: 0.023728\n",
      "Iteration 84000, cost: 0.023725\n",
      "Iteration 85000, cost: 0.023723\n",
      "Iteration 86000, cost: 0.023721\n",
      "Iteration 87000, cost: 0.023719\n",
      "Iteration 88000, cost: 0.023716\n",
      "Iteration 89000, cost: 0.023714\n",
      "Iteration 90000, cost: 0.023712\n",
      "Iteration 91000, cost: 0.023709\n",
      "Iteration 92000, cost: 0.023706\n",
      "Iteration 93000, cost: 0.023704\n",
      "Iteration 94000, cost: 0.023702\n",
      "Iteration 95000, cost: 0.023700\n",
      "Iteration 96000, cost: 0.023699\n",
      "Iteration 97000, cost: 0.023698\n",
      "Iteration 98000, cost: 0.023696\n",
      "Iteration 99000, cost: 0.023695\n",
      "Iteration 100000, cost: 0.023693\n",
      "Iteration 101000, cost: 0.023692\n",
      "Iteration 102000, cost: 0.023690\n",
      "Iteration 103000, cost: 0.023688\n",
      "Iteration 104000, cost: 0.023685\n",
      "Iteration 105000, cost: 0.023682\n",
      "Iteration 106000, cost: 0.023679\n",
      "Iteration 107000, cost: 0.023676\n",
      "Iteration 108000, cost: 0.023673\n",
      "Iteration 109000, cost: 0.023671\n",
      "Iteration 110000, cost: 0.023669\n",
      "Iteration 111000, cost: 0.023668\n",
      "Iteration 112000, cost: 0.023667\n",
      "Iteration 113000, cost: 0.023666\n",
      "Iteration 114000, cost: 0.023665\n",
      "Iteration 115000, cost: 0.023665\n",
      "Iteration 116000, cost: 0.023664\n",
      "Iteration 117000, cost: 0.023663\n",
      "Iteration 118000, cost: 0.023663\n",
      "Iteration 119000, cost: 0.023662\n",
      "Iteration 120000, cost: 0.023662\n",
      "Iteration 121000, cost: 0.023661\n",
      "Iteration 122000, cost: 0.023660\n",
      "Iteration 123000, cost: 0.023659\n",
      "Iteration 124000, cost: 0.023659\n",
      "Iteration 125000, cost: 0.023658\n",
      "Iteration 126000, cost: 0.023658\n",
      "Iteration 127000, cost: 0.023657\n",
      "Iteration 128000, cost: 0.023656\n",
      "Iteration 129000, cost: 0.023656\n",
      "Iteration 130000, cost: 0.023655\n",
      "Iteration 131000, cost: 0.023654\n",
      "Iteration 132000, cost: 0.023654\n",
      "Iteration 133000, cost: 0.023653\n",
      "Iteration 134000, cost: 0.023652\n",
      "Iteration 135000, cost: 0.023652\n",
      "Iteration 136000, cost: 0.023651\n",
      "Iteration 137000, cost: 0.023651\n",
      "Iteration 138000, cost: 0.023650\n",
      "Iteration 139000, cost: 0.023649\n",
      "Iteration 140000, cost: 0.023649\n",
      "Iteration 141000, cost: 0.023648\n",
      "Iteration 142000, cost: 0.023647\n",
      "Iteration 143000, cost: 0.023646\n",
      "Iteration 144000, cost: 0.023645\n",
      "Iteration 145000, cost: 0.023643\n",
      "Iteration 146000, cost: 0.023641\n",
      "Iteration 147000, cost: 0.023637\n",
      "Iteration 148000, cost: 0.023630\n",
      "Iteration 149000, cost: 0.023614\n",
      "Iteration 150000, cost: 0.023566\n",
      "Iteration 151000, cost: 0.023310\n",
      "Iteration 152000, cost: 0.022500\n",
      "Iteration 153000, cost: 0.022493\n",
      "Iteration 154000, cost: 0.022490\n",
      "Iteration 155000, cost: 0.022489\n",
      "Iteration 156000, cost: 0.022486\n",
      "Iteration 157000, cost: 0.022483\n",
      "Iteration 158000, cost: 0.022481\n",
      "Iteration 159000, cost: 0.022479\n",
      "Iteration 160000, cost: 0.022478\n",
      "Iteration 161000, cost: 0.022477\n",
      "Iteration 162000, cost: 0.022476\n",
      "Iteration 163000, cost: 0.022475\n",
      "Iteration 164000, cost: 0.022475\n",
      "Iteration 165000, cost: 0.022474\n",
      "Iteration 166000, cost: 0.022473\n",
      "Iteration 167000, cost: 0.022472\n",
      "Iteration 168000, cost: 0.022472\n",
      "Iteration 169000, cost: 0.022471\n",
      "Iteration 170000, cost: 0.022471\n",
      "Iteration 171000, cost: 0.022469\n",
      "Iteration 172000, cost: 0.022469\n",
      "Iteration 173000, cost: 0.022468\n",
      "Iteration 174000, cost: 0.022468\n",
      "Iteration 175000, cost: 0.022467\n",
      "Iteration 176000, cost: 0.022467\n",
      "Iteration 177000, cost: 0.022467\n",
      "Iteration 178000, cost: 0.022465\n",
      "Iteration 179000, cost: 0.022465\n",
      "Iteration 180000, cost: 0.022464\n",
      "Iteration 181000, cost: 0.022463\n",
      "Iteration 182000, cost: 0.022464\n",
      "Iteration 183000, cost: 0.022462\n",
      "Iteration 184000, cost: 0.022462\n",
      "Iteration 185000, cost: 0.022462\n",
      "Iteration 186000, cost: 0.022460\n",
      "Iteration 187000, cost: 0.022459\n",
      "Iteration 188000, cost: 0.022459\n",
      "Iteration 189000, cost: 0.022459\n",
      "Iteration 190000, cost: 0.022458\n",
      "Iteration 191000, cost: 0.022457\n",
      "Iteration 192000, cost: 0.022456\n",
      "Iteration 193000, cost: 0.022457\n",
      "Iteration 194000, cost: 0.022456\n",
      "Iteration 195000, cost: 0.022454\n",
      "Iteration 196000, cost: 0.022454\n",
      "Iteration 197000, cost: 0.022453\n",
      "Iteration 198000, cost: 0.022454\n",
      "Iteration 199000, cost: 0.022452\n",
      "Iteration 200000, cost: 0.022451\n",
      "Iteration 201000, cost: 0.022450\n",
      "Iteration 202000, cost: 0.022451\n",
      "Iteration 203000, cost: 0.022448\n",
      "Iteration 204000, cost: 0.022446\n",
      "Iteration 205000, cost: 0.022446\n",
      "Iteration 206000, cost: 0.022445\n",
      "Iteration 207000, cost: 0.022445\n",
      "Iteration 208000, cost: 0.022445\n",
      "Iteration 209000, cost: 0.022444\n"
     ]
    }
   ],
   "source": [
    "# Two layer, 50 neurons, sigmoid activation function, plain gradient descent, rmsprop and adam, l1\n",
    "layer_output_sizes = [50, 50, 1]\n",
    "activation_funcs = [sigmoid, sigmoid, identity]\n",
    "activation_ders = [sigmoid_der, sigmoid_der, identity_der]\n",
    "\n",
    "scaler.fit(x_train)\n",
    "x_train_s = scaler.transform(x_train)\n",
    "x_test_s = scaler.transform(x_test)\n",
    "inputs = x_train_s\n",
    "targets = y_train\n",
    "\n",
    "for option in ['RMSProp', 'ADAM']:\n",
    "    targets = y_train\n",
    "    NN = NeuralNetwork(\n",
    "        x_train_s,\n",
    "        targets,\n",
    "        layer_output_sizes,\n",
    "        activation_funcs,\n",
    "        activation_ders,\n",
    "        MSE,\n",
    "        mse_der,\n",
    "        L1 = True,\n",
    "        lmbda = 0.001,\n",
    "    )\n",
    "\n",
    "    NN.train_network_plain_gd(max_iter=1000000000, lr_method=option)\n",
    "\n",
    "    targets = y_test\n",
    "\n",
    "    predictions = NN.predict(x_test_s)\n",
    "    print(f'NN mse ({option}):', MSE(predictions, targets))\n",
    "    print(f'NN R2 ({option}):', R2(predictions, targets))\n",
    "\n",
    "    mse = MSE(predictions, targets)\n",
    "    r2 = R2(predictions, targets)\n",
    "\n",
    "    metrics = {\"mse\": float(mse), \"r2\": float(r2)}\n",
    "    with open(f\"nn_50_sigmoid_{option.lower()}_l1_lmbda_0_001_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fys-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
